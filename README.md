# ðŸ§  Understanding Customer â€” Intent Classification

This project introduces **Deep Learning for Natural Language Processing (NLP)** with a focus on **Intent Classification** â€” identifying what a user wants based on their text input.  
Example:  
> â€œIs it raining?â€ â†’ intent: *ask about weather*

---

## ðŸš€ Overview

Youâ€™ll explore how NLP models evolved from **rule-based chatbots** like ELIZA to modern **Transformer-based** architectures such as **BERT**.

This project provides a **hands-on experience** with:
- **Recurrent Neural Networks (RNN)**
- **Long Short-Term Memory (LSTM)**
- **Transformers**
- **BERT (Bidirectional Encoder Representations from Transformers)**

and introduces key NLP concepts such a

---

## Chapter I â€“ Preamble

The journey of conversational AI began with **ELIZA**, a 1960s chatbot that simulated a therapist.  
ELIZA worked by identifying **keywords** and applying **transformation rules** to generate responses.

Todayâ€™s chatbots follow similar principles:
1. **Classify user intent** â€“ What does the user want?  
2. **Generate or retrieve a relevant response.**

Example:  
> â€œWhatâ€™s the temperature right now?â€ and â€œIs it raining?â€ â†’ both map to the same intent: *ask about weather.*

---

## Chapter II â€“ Introduction

### ðŸ§  Why Specialized Architectures for Text?

Unlike tabular data, text is **sequential** â€” order matters.  
Deep learning offers several architectures for handling text:

- **RNN:** Processes sequences but forgets long-term context.  
- **LSTM:** Adds â€œmemory gatesâ€ to retain context.  
- **Transformer:** Uses an **attention mechanism** to process sequences in parallel, understanding context globally.  
- **BERT:** A bidirectional Transformer pre-trained for deep contextual understanding.

Training these models from scratch is expensive, so we use **transfer learning** â€” fine-tuning pre-trained models on our data for powerful results.

---

## Chapter III â€“ Rules of Project

- Use **Python 3**.  
- You can organize your files freely, but keep the structure clean.  
- You may use **Google Colab** for training (GPU recommended).  
- Place datasets inside the `data/` folder.  
- Keep a **research journal** to track model changes, hyperparameters, and metrics.

---

## Chapter IV â€“ Instructions

1. Use any framework: **PyTorch**, **TensorFlow**, or **Keras**.  
2. Save your trained model in **Pickle (.pkl)** format.  
3. The notebook(s) must include:
   - Data preprocessing  
   - Model training (RNN, LSTM, BERT)  
   - Evaluation (accuracy â‰¥ 0.8 on test set)  
4. Predictions should be saved to `intents.csv`.  

> **Fallback Intent:**  
> Include handling for unseen queries (e.g., â€œI couldnâ€™t understand you.â€).  
> This improves chatbot robustness and evaluation score.

---

## ðŸ“Š Example Folder Structure
```
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ intents_train.csv
â”‚ â”œâ”€â”€ intents_test.csv
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ rnn_model.ipynb
â”‚ â”œâ”€â”€ lstm_model.ipynb
â”‚ â”œâ”€â”€ bert_model.ipynb
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ rnn.pkl
â”‚ â”œâ”€â”€ lstm.pkl
â”‚ â”œâ”€â”€ bert.pkl
â”œâ”€â”€ intents.csv
â”œâ”€â”€ research_journal.md
â””â”€â”€ README.md
```