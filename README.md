# Understanding Customer â€” Intent Classification

This project introduces **Deep Learning for Natural Language Processing (NLP)** with a focus on **Intent Classification** â€” identifying what a user wants based on their text input.  
Example:  
> â€œIs it raining?â€ â†’ intent: *ask about weather*


## ğŸ§© Overview

We explore how NLP models evolved from **rule-based chatbots** like ELIZA to modern **Transformer-based** architectures such as **BERT**.

This project offers **hands-on experience** with key deep learning architectures for text understanding:
- **Recurrent Neural Networks (RNN)**
- **Long Short-Term Memory (LSTM)** networks
- **Transformers**
- **BERT (Bidirectional Encoder Representations from Transformers)**

It also introduces essential NLP concepts such as **sequence modeling**, **context representation**, and **transfer learning**.


## ğŸ’¬ Preamble

The journey of conversational AI began with **ELIZA**, a 1960s chatbot that simulated a therapist.  
ELIZA worked by identifying **keywords** and applying **transformation rules** to generate responses.

Todayâ€™s chatbots follow similar principles:
1. **Classify user intent** â€“ What does the user want?  
2. **Generate or retrieve a relevant response.**

Example:  
> â€œWhatâ€™s the temperature right now?â€ and â€œIs it raining?â€ â†’ both map to the same intent: *ask about weather.*


## ğŸ” Introduction

### Why Specialized Architectures for Text?

Unlike tabular data, text is **sequential** â€” word order and context matter.  
Deep learning provides several architectures that effectively model sequences:

- **RNN:** Processes sequences step by step but struggles with long-term dependencies.  
- **LSTM:** Introduces *memory gates* to retain and control contextual information.  
- **Transformer:** Uses an **attention mechanism** to process entire sequences in parallel, improving speed and context handling.  
- **BERT:** A **bidirectional Transformer** that learns deep contextual representations of language.

Training such models from scratch is computationally expensive, so we leverage **transfer learning** â€” fine-tuning pre-trained models on domain-specific data for powerful results.


## ğŸ“‚ Project Structure
```
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ intents_train.csv
â”‚ â”œâ”€â”€ intents_test.csv
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ rnn_model.ipynb
â”‚ â”œâ”€â”€ lstm_model.ipynb
â”‚ â”œâ”€â”€ bert_model.ipynb
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ rnn.pkl
â”‚ â”œâ”€â”€ lstm.pkl
â”‚ â”œâ”€â”€ bert.pkl
â”œâ”€â”€ intents.csv
â”œâ”€â”€ research_journal.md
â””â”€â”€ README.md
```